[
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/description(indent:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from IndentedDescription.description(indent:). ",
    "summary" : "Inherited from IndentedDescription.description(indent:).",
    "title" : "description(indent:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/description"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from IndentedDescription.description. ",
    "summary" : "Inherited from IndentedDescription.description.",
    "title" : "description"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SequentialBuilder\/buildArray(_:)-j82o"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "buildArray(_:)"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SinusoidalPositionalEncoding"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Implements sinusoidal positional encoding.  Overview For more details see the paper “Attention Is All You Need” https:\/\/arxiv.org\/abs\/1706.03762. See Also Positional Encoding",
    "summary" : "Implements sinusoidal positional encoding.",
    "title" : "SinusoidalPositionalEncoding"
  },
  {
    "headings" : [
      "Discussion"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Sequential\/init(layers:)-43yu"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "A convenient way to write code that builds a Sequential layer:  Discussion  produces: ",
    "summary" : "A convenient way to write code that builds a Sequential layer:",
    "title" : "init(layers:)"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/HardSwish"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the hardswish function, element-wise  Overview This is:  See Also Activation Functions and Layers hardSwish(_:)",
    "summary" : "Applies the hardswish function, element-wise",
    "title" : "HardSwish"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Sigmoid"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the element-wise logistic sigmoid.  Overview For details, please see this documentation This is:  See Also Activation Functions and Layers sigmoid(_:)",
    "summary" : "Applies the element-wise logistic sigmoid.",
    "title" : "Sigmoid"
  },
  {
    "headings" : [
      "Overview"
    ],
    "kind" : "article",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/layers"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Built-in layers. Overview MLXNN provides a number of built-in layers that can be used to build models.\nSee also Activation Functions and Layers for Activation Layers and Creating Modules for examples of their use",
    "summary" : "Built-in layers.",
    "title" : "Layers"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LogSoftMax"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Log Softmax function.  Overview This is:  See Also Activation Functions and Layers logSoftMax(_:axis:)",
    "summary" : "Applies the Log Softmax function.",
    "title" : "LogSoftMax"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Transformer"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Implements a standard Transformer model.  Overview The implementation is based on “Attention Is All You Need” https:\/\/arxiv.org\/abs\/1706.03762. The Transformer model contains an encoder and a decoder. The encoder processes the input sequence and the decoder generates the output sequence. The interaction between encoder and decoder happens through the attention mechanism. See Also Transformer https:\/\/arxiv.org\/abs\/1706.03762",
    "summary" : "Implements a standard Transformer model.",
    "title" : "Transformer"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/mish(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Mish function, element-wise.  Discussion Mish: A Self Regularized Non-Monotonic Neural Activation Function. Reference: https:\/\/arxiv.org\/abs\/1908.08681 This is:  See Also Activation Functions and Layers Mish",
    "summary" : "Applies the Mish function, element-wise.",
    "title" : "mish(_:)"
  },
  {
    "headings" : [
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/namedModules()"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Return a flat array of all the Module in the instance (including self) with their keys.  See Also modules() children() leafModules()",
    "summary" : "Return a flat array of all the Module in the instance (including self) with their keys.",
    "title" : "namedModules()"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/GLU\/init(axis:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "init(axis:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/intersection(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from OptionSet.intersection(_:). ",
    "summary" : "Inherited from OptionSet.intersection(_:).",
    "title" : "intersection(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/isSubset(of:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from SetAlgebra.isSubset(of:). ",
    "summary" : "Inherited from SetAlgebra.isSubset(of:).",
    "title" : "isSubset(of:)"
  },
  {
    "headings" : [
      "Return Value",
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/mapParameters(map:isLeaf:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Apply a map to all parameters (ModuleValue.parameters(_:)) in the module and its children.  closure that transforms MLXArray into Result type or nil optional leaf function Return Value NestedDictionary of mapped results Discussion For example:  This is equivalent to:  See Also Module Filter and Map Functions mapParameters(map:)",
    "summary" : "Apply a map to all parameters (ModuleValue.parameters(_:)) in the module and its children.",
    "title" : "mapParameters(map:isLeaf:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Sigmoid\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "Overview",
      "Examples"
    ],
    "kind" : "article",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/module-filters"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Pre-built filter and map functions in Module. Overview Module provides a number of pre-build filter and map functions for use in: filterMap(filter:map:isLeaf:) apply(filter:map:) mapParameters(map:isLeaf:) See those methods for more information. Examples The filterMap() method has several options for controlling the traversal of the modules, parameters and other values in the model.  Here is an example that limits the traversal to just local parameters and produces a NestedDictionary of the shapes:  Applying a map to the entire set of parameters (though some traversal control is possible through the optional isLeaf) is very easy:  Finally, apply() does both a filter and an update(parameters:). This code would convert all floating point parameters to .float16. ",
    "summary" : "Pre-built filter and map functions in Module.",
    "title" : "Module Filter and Map Functions"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Dropout3d\/init(p:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "init(p:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/visit(modules:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "visit(modules:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SELU\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/MultiHeadAttention"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Implements the scaled dot product attention with multiple heads.  See Also Transformer init(dimensions:numHeads:queryInputDimensions:keyInputDimensions:valueInputDimensions:valueDimensions:valueOutputDimensions:bias:)",
    "summary" : "Implements the scaled dot product attention with multiple heads.",
    "title" : "MultiHeadAttention"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Mish\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "Return Value",
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/klDivLoss(inputs:targets:axis:reduction:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Computes the Kullback-Leibler divergence loss.  Log probabilities for the predicted distribution Log probabilities for the target distribution distribution axis reduction type Return Value computed Kullback-Leibler divergence loss Discussion Computes the following when the reduction: .none:  See Also Loss Functions",
    "summary" : "Computes the Kullback-Leibler divergence loss.",
    "title" : "klDivLoss(inputs:targets:axis:reduction:)"
  },
  {
    "headings" : [
      "Discussion",
      "References"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LayerNorm\/init(dimensions:eps:affine:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies layer normalization [1] on the inputs.  number of features in the input value added to the denominator for numerical stability if true adds a trainable weight and bias Discussion See LayerNorm python docs for more information. References https:\/\/arxiv.org\/abs\/1607.06450",
    "summary" : "Applies layer normalization [1] on the inputs.",
    "title" : "init(dimensions:eps:affine:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/init()"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from OptionSet.init(). ",
    "summary" : "Inherited from OptionSet.init().",
    "title" : "init()"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/all"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "all"
  },
  {
    "headings" : [
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/filterOther"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Filter that will accept all structure (.array and .dictionary) and ModuleValue.other(_:).  See Also Module Filter and Map Functions",
    "summary" : "Filter that will accept all structure (.array and .dictionary) and ModuleValue.other(_:).",
    "title" : "filterOther"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SoftPlus\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/isStrictSubset(of:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from SetAlgebra.isStrictSubset(of:). ",
    "summary" : "Inherited from SetAlgebra.isStrictSubset(of:).",
    "title" : "isStrictSubset(of:)"
  },
  {
    "headings" : [
      "Return Value",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/binaryCrossEntropy(logits:targets:reduction:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Computes the binary cross entropy loss.  unnormalized predicted logits binary target values in {0, 1} reduction type Return Value computed binary cross entropy loss See Also Loss Functions",
    "summary" : "Computes the binary cross entropy loss.",
    "title" : "binaryCrossEntropy(logits:targets:reduction:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Sequential\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "Discussion",
      "References"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/GroupNorm\/init(groupCount:dimensions:eps:affine:pytorchCompatible:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies Group Normalization [1] on the inputs.  number of groups to separate the features into number of features in the input value added to the denominator for numerical stability if true adds a trainable weight and bias if true perform the group normalization in the same order\/grouping as PyTorch Discussion See GroupNorm python docs for more information. The feature dimension is assumed to be the last dimension and the dimensions that precede it (except the first) are considered the spatial dimensions. References https:\/\/arxiv.org\/abs\/1803.08494",
    "summary" : "Applies Group Normalization [1] on the inputs.",
    "title" : "init(groupCount:dimensions:eps:affine:pytorchCompatible:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/train(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Recursively set the model’s training mode.  Discussion Training mode only applies to certain layers. For example Dropout applies a random mask in training mode, but is the identity in evaluation mode. See Also training",
    "summary" : "Recursively set the model’s training mode.",
    "title" : "train(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "article",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/SetAlgebra-Implementations"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "",
    "summary" : "",
    "title" : "SetAlgebra Implementations"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/RoPE\/init(dimensions:traditional:base:scale:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Initialize RoPE.  The feature dimensions to be rotated. If the input feature is larger than dims then the rest is left unchanged If true choose the traditional implementation which is slightly less efficient The base used to compute angular frequency for each dimension in the positional encodings scale used to scale the positions",
    "summary" : "Initialize RoPE.",
    "title" : "init(dimensions:traditional:base:scale:)"
  },
  {
    "headings" : [

    ],
    "kind" : "article",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LossReduction\/Equatable-Implementations"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "",
    "summary" : "",
    "title" : "Equatable Implementations"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LossReduction\/init(rawValue:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from RawRepresentable.init(rawValue:). ",
    "summary" : "Inherited from RawRepresentable.init(rawValue:).",
    "title" : "init(rawValue:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ModuleInfo\/init(key:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "init(key:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/MultiHeadAttention\/createAdditiveCausalMask(_:dtype:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Creates an attention mask for use with callAsFunction(_:keys:values:mask:)  number of dimensions data type of the mask",
    "summary" : "Creates an attention mask for use with callAsFunction(_:keys:values:mask:)",
    "title" : "createAdditiveCausalMask(_:dtype:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/GroupNorm\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ParameterInfo\/wrappedValue"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "wrappedValue"
  },
  {
    "headings" : [
      "Discussion"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Conv2d\/init(inputChannels:outputChannels:kernelSize:stride:padding:bias:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies a 2-dimensional convolution over the multi-channel input image.  number of input channels (C from the discussion) number of output channels size of the convolution filters stride when applying the filter many positions to 0-pad the input with if true add a learnable bias to the output Discussion The channels are expected to be last i.e. the input shape should be NHWC where: N is the batch dimension H is the input image height W is the input image width C is the number of input channels",
    "summary" : "Applies a 2-dimensional convolution over the multi-channel input image.",
    "title" : "init(inputChannels:outputChannels:kernelSize:stride:padding:bias:)"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SiLU"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Sigmoid Linear Unit. Also known as Swish.  Overview This is:  See Also Activation Functions and Layers silu(_:)",
    "summary" : "Applies the Sigmoid Linear Unit. Also known as Swish.",
    "title" : "SiLU"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LossReduction\/hashValue"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from RawRepresentable.hashValue. ",
    "summary" : "Inherited from RawRepresentable.hashValue.",
    "title" : "hashValue"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/valueAndGrad(model:_:)-548r7"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Variant of valueAndGrad(model:_:) that can be used to pass an arbitrary number of MLXArray to a loss function.  Discussion For example, given a loss function:  it can be wrapped as:  See Also Training a Model valueAndGrad(model:_:)",
    "summary" : "Variant of valueAndGrad(model:_:) that can be used to pass an arbitrary number of MLXArray to a loss function.",
    "title" : "valueAndGrad(model:_:)"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ParameterInfo"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "ParameterInfo allows you to specify alternate keys for parameter propreties.  Overview For example:  will have keys weights and bias. See Also Creating Modules ModuleInfo",
    "summary" : "ParameterInfo allows you to specify alternate keys for parameter propreties.",
    "title" : "ParameterInfo"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Identity"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "A placeholder identity operator that is argument-insensitive. ",
    "summary" : "A placeholder identity operator that is argument-insensitive.",
    "title" : "Identity"
  },
  {
    "headings" : [

    ],
    "kind" : "article",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/Equatable-Implementations"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "",
    "summary" : "",
    "title" : "Equatable Implementations"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/init(rawValue:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from OptionSet.init(rawValue:). ",
    "summary" : "Inherited from OptionSet.init(rawValue:).",
    "title" : "init(rawValue:)"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SoftPlus"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Softplus function.  Overview This is:  See Also Activation Functions and Layers softPlus(_:)",
    "summary" : "Applies the Softplus function.",
    "title" : "SoftPlus"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/innerState()"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from Updatable.innerState(). ",
    "summary" : "Inherited from Updatable.innerState().",
    "title" : "innerState()"
  },
  {
    "headings" : [
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Conv1d"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies a 1-dimensional convolution over the multi-channel input sequence.  See Also Conv2d init(inputChannels:outputChannels:kernelSize:stride:padding:bias:)",
    "summary" : "Applies a 1-dimensional convolution over the multi-channel input sequence.",
    "title" : "Conv1d"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Dropout"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Randomly zero a portion of the elements during training.  Overview The remaining elements are multiplied with 1 \/ (1-p) where p is the probability of zeroing an element. This is done so the expected value of a given element will remain the same. See Also Dropout2d Dropout3d",
    "summary" : "Randomly zero a portion of the elements during training.",
    "title" : "Dropout"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/QuantizedLinear"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies an affine transformation to the input using a quantized weight matrix.  Overview It is the quantized equivalent of Linear.  For now its parameters are frozen and will not be included in any gradient computation but this will probably change in the future. QuantizedLinear also provides several useful static to convert linear layers to QuantizedLinear layers. from(linear:groupSize:bits:) – returns a QuantizedLinear that applies the same linear transformation up to the quantization error quantize(model:groupSize:bits:predicate:) – swaps all the linear layers of the module with QuantizedLinear ones Please see the disucssion in Linear for considerations when replacing layers. See Also init(weight:bias:groupSize:bits:)",
    "summary" : "Applies an affine transformation to the input using a quantized weight matrix.",
    "title" : "QuantizedLinear"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SinusoidalPositionalEncoding\/init(dimensions:minFrequency:maxFrequency:scale:cosineFirst:fullTurns:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Initialize the layer.  dimensionality of the resulting positional embeddings minimum frequency expected maximum frequency expected multiplicative scale for the embeddings.  Default is sqrt(dimensions \/ 2) if true embed using [cos(x), sin(x)] instead of the reverse if true multiply the frequencies with 2 * pi",
    "summary" : "Initialize the layer.",
    "title" : "init(dimensions:minFrequency:maxFrequency:scale:cosineFirst:fullTurns:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ParameterInfo\/init(wrappedValue:key:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "init(wrappedValue:key:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SequentialBuilder\/buildPartialBlock(accumulated:next:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "buildPartialBlock(accumulated:next:)"
  },
  {
    "headings" : [

    ],
    "kind" : "article",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/GELU\/Approximation\/Equatable-Implementations"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "",
    "summary" : "",
    "title" : "Equatable Implementations"
  },
  {
    "headings" : [
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Conv2d"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies a 2-dimensional convolution over the multi-channel input image.  See Also Conv1d init(inputChannels:outputChannels:kernelSize:stride:padding:bias:)",
    "summary" : "Applies a 2-dimensional convolution over the multi-channel input image.",
    "title" : "Conv2d"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/RMSNorm\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/valueAndGrad(model:_:)-45dg5"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Variant of valueAndGrad(model:_:) that can be used to pass an arbitrary parameters to a loss function.  Discussion Prefer the other valueAndGrad(model:_:) functions if the arguments are all MLXArray. This variant requires re-constructing the gradient function per call. For example, given a loss function:  it can be wrapped as:  See Also Training a Model valueAndGrad(model:_:)",
    "summary" : "Variant of valueAndGrad(model:_:) that can be used to pass an arbitrary parameters to a loss function.",
    "title" : "valueAndGrad(model:_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/QuantizedLinear\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Embedding\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/hardSwish(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the hardswish function, element-wise  Discussion This is:  See Also Activation Functions and Layers HardSwish",
    "summary" : "Applies the hardswish function, element-wise",
    "title" : "hardSwish(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LossReduction\/reduce(loss:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "reduce(loss:)"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LogSigmoid"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Log Sigmoid function.  Overview This is:  See Also Activation Functions and Layers logSigmoid(_:)",
    "summary" : "Applies the Log Sigmoid function.",
    "title" : "LogSigmoid"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SoftSign\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/GELU\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Conv1d\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ModuleItems"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "NestedDictionary structure of ModuleValue from items() ",
    "summary" : "NestedDictionary structure of ModuleValue from items()",
    "title" : "ModuleItems"
  },
  {
    "headings" : [
      "Return Value",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/nllLoss(inputs:targets:axis:reduction:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Computes the negative log likelihood loss.  predicted distribution in log space the target values distribution axis reduction type Return Value computed NLL loss See Also Loss Functions",
    "summary" : "Computes the negative log likelihood loss.",
    "title" : "nllLoss(inputs:targets:axis:reduction:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ModuleInfo\/init(wrappedValue:key:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "init(wrappedValue:key:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ReLU6\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/GLU"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the gated linear unit function.  Overview This function splits the axis dimension of the input into two halves (a and b) and applies a * sigmoid(b). See Also Activation Functions and Layers glu(_:axis:)",
    "summary" : "Applies the gated linear unit function.",
    "title" : "GLU"
  },
  {
    "headings" : [
      "Discussion"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/update(parameters:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "A non-throwing version of update(parameters:verify:).  Discussion This passes verify: .none.  Note that there may still be fatalErrors() if for example an MLXArray is set on a Module.",
    "summary" : "A non-throwing version of update(parameters:verify:).",
    "title" : "update(parameters:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/selu(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Scaled Exponential Linear Unit.  Discussion This is:  See Also Activation Functions and Layers SELU elu(_:alpha:)",
    "summary" : "Applies the Scaled Exponential Linear Unit.",
    "title" : "selu(_:)"
  },
  {
    "headings" : [
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/filterLocalParameters"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Filter that will accept all structure (.array and .dictionary) and ModuleValue.parameters(_:) without allowing recursion into sub-Modules (layers).  See Also Module Filter and Map Functions filterValidParameters filterTrainableParameters",
    "summary" : "Filter that will accept all structure (.array and .dictionary) and ModuleValue.parameters(_:) without allowing recursion into sub-Modules (layers).",
    "title" : "filterLocalParameters"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/isEmpty"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from SetAlgebra.isEmpty. ",
    "summary" : "Inherited from SetAlgebra.isEmpty.",
    "title" : "isEmpty"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/leakyRelu(_:negativeSlope:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Leaky Rectified Linear Unit.  Discussion This is:  See Also Activation Functions and Layers LeakyReLU",
    "summary" : "Applies the Leaky Rectified Linear Unit.",
    "title" : "leakyRelu(_:negativeSlope:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/mapModule(map:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Function that will turn a (Module) -> Result? into a function suitable for use in filterMap(filter:map:isLeaf:).  Discussion For example:  See Also Module Filter and Map Functions ModuleValue.module(_:) filterMap(filter:map:isLeaf:)",
    "summary" : "Function that will turn a (Module) -> Result? into a function suitable for use in filterMap(filter:map:isLeaf:).",
    "title" : "mapModule(map:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/relu6(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Rectified Linear Unit 6.  Discussion This is:  See Also Activation Functions and Layers ReLU6",
    "summary" : "Applies the Rectified Linear Unit 6.",
    "title" : "relu6(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/InstanceNorm\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Embedding\/describeExtra(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Describe the shape of the weight. ",
    "summary" : "Describe the shape of the weight.",
    "title" : "describeExtra(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/UnaryLayer\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "Overview"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Sequential"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "A layer that calls the passed UnaryLayer in order.  Overview Sequential can be constructed either with an array of layers or using a SequentialBuilder:  produces: ",
    "summary" : "A layer that calls the passed UnaryLayer in order.",
    "title" : "Sequential"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ReLU"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Rectified Linear Unit.  Overview This is:  See Also Activation Functions and Layers relu(_:)",
    "summary" : "Applies the Rectified Linear Unit.",
    "title" : "ReLU"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/gelu(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Gaussian Error Linear Units function.  Discussion This is:  See Also Activation Functions and Layers GELU geluApproximate(_:) geluFastApproximate(_:)",
    "summary" : "Applies the Gaussian Error Linear Units function.",
    "title" : "gelu(_:)"
  },
  {
    "headings" : [
      "References",
      "See also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/InstanceNorm"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies instance normalization [1] on the inputs.  References https:\/\/arxiv.org\/abs\/1607.08022 See also Normalization",
    "summary" : "Applies instance normalization [1] on the inputs.",
    "title" : "InstanceNorm"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ModuleValue\/none"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "ModuleValue.none"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/GELU"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Gaussian Error Linear Units function.  Overview There are three variations: GELU.Approximation.none GELU.Approximation.precise GELU.Approximation.fast See Also Activation Functions and Layers gelu(_:) geluApproximate(_:) geluFastApproximate(_:)",
    "summary" : "Applies the Gaussian Error Linear Units function.",
    "title" : "GELU"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LogSigmoid\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Transformer\/init(dimensions:numHeads:encoderLayerCount:decoderLayerCount:mlpDimensions:dropout:activation:normFirst:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Initialize the transformer.  number of expected features in the encoder\/decoder number of attention heads number of layers in the encoder number of layers in the decoder hidden dimensions of the MLP block in each layer.  Defaults to 4 * dimensions if not specified dropout value for the encode and decoder.  Dropout is used after each attention layer and the activation in the MLP layer the activation layer for the MLP hidden layer if true encode and decoder layers will perform layer normalization before attention and MLP operations, otherwise after",
    "summary" : "Initialize the transformer.",
    "title" : "init(dimensions:numHeads:encoderLayerCount:decoderLayerCount:mlpDimensions:dropout:activation:normFirst:)"
  },
  {
    "headings" : [
      "Discussion"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ModuleValue\/parameters(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "An MLXArray parameters value.  Discussion From code: ",
    "summary" : "An MLXArray parameters value.",
    "title" : "ModuleValue.parameters(_:)"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/RoPE"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Implements the rotary positional encoding.  Overview The traditional implementation rotates consecutive pairs of elements in the feature dimension while the default implementation rotates pairs with stride half the feature dimensions for efficiency. For more details see RoFormer: Enhanced Transformer with Rotary Position Embedding (https:\/\/arxiv.org\/abs\/2104.09864) See Also Positional Encoding",
    "summary" : "Implements the rotary positional encoding.",
    "title" : "RoPE"
  },
  {
    "headings" : [
      "Discussion"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ModuleValue\/module(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "A module value.  Discussion From code: ",
    "summary" : "A module value.",
    "title" : "ModuleValue.module(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Sequential\/init(layers:)-3zdqn"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "init(layers:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ALiBi\/init()"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from Module.init(). ",
    "summary" : "Inherited from Module.init().",
    "title" : "init()"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/RMSNorm\/describeExtra(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Describe dimensions and eps. ",
    "summary" : "Describe dimensions and eps.",
    "title" : "describeExtra(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/GELU\/Approximation\/!=(_:_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from Equatable.!=(_:_:). ",
    "summary" : "Inherited from Equatable.!=(_:_:).",
    "title" : "!=(_:_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/insert(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from OptionSet.insert(_:). ",
    "summary" : "Inherited from OptionSet.insert(_:).",
    "title" : "insert(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ModuleParameters"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "NestedDictionary structure of MLXArray ",
    "summary" : "NestedDictionary structure of MLXArray",
    "title" : "ModuleParameters"
  },
  {
    "headings" : [
      "Discussion"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/QuantizedLinear\/quantize(model:groupSize:bits:predicate:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Replace Linear layers with QuantizedLinear.  the model to update The group size to use for the quantized weight The bit width to use for the quantized weight optional predicate for identifying layers to change – default finds all Linear layers Discussion Please see the disucssion in Linear for considerations when replacing layers.",
    "summary" : "Replace Linear layers with QuantizedLinear.",
    "title" : "quantize(model:groupSize:bits:predicate:)"
  },
  {
    "headings" : [
      "Overview"
    ],
    "kind" : "article",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/training"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "A model training loop. Overview The model traing loop in MLX consists of: defining a model defining a loss function that measures the distance between predicted and expected values using the valueAndGrad(model:_:) function to create a new function to compute the gradient presenting training data and expected values to the model, measuring the loss and computing the gradient using an optimizer to apply the gradient to the model parameters see more about optimizers in MLXOptimizers repeat Here is an example showing a simple model that learns a linear function, literally f(x) = mx + b.  This model is simpler than most, but it is easy to understand and see how it works.  Next we define a loss function – there are a number of Loss Functions available to use.  I chose one that accepted simple predictions and targets:  Now we create the model, build the lg (loss and gradient) function and create the optimizer.  We could define any f(x) – I will use a simple one that the model should be able to match very closely.  Now we run the training loop for a number of epochs.  In each epoch we produce training data (input x values) and expected values (just evaluate f(x)). From this we can evaluate the model and compute a loss and gradient. The gradients are given to the optimizer to update the model parameters. ",
    "summary" : "A model training loop.",
    "title" : "Training a Model"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Options for verifying update(parameters:verify:) and update(modules:verify:). ",
    "summary" : "Options for verifying update(parameters:verify:) and update(modules:verify:).",
    "title" : "Module.VerifyUpdate"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LossReduction\/sum"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "take the sum of the loss.  This produces a a scalar array. ",
    "summary" : "take the sum of the loss.  This produces a a scalar array.",
    "title" : "LossReduction.sum"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/HardSwish\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "Return Value",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/cosineSimilarityLoss(x1:x2:axis:eps:reduction:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Computes the cosine similarity between the two inputs.  first array second array embedding axis minimum value of the denominator used for numerical stability reduction type Return Value computed cosine similarity loss See Also Loss Functions",
    "summary" : "Computes the cosine similarity between the two inputs.",
    "title" : "cosineSimilarityLoss(x1:x2:axis:eps:reduction:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SoftMax\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/describeExtra(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Describe extra parameters.  Discussion This will print a description of ModuleValue.other(_:) ivars, e.g.:  Subclasses can override this to print custom information, e.g. shape information derived from parameters:  See Also description(indent:)",
    "summary" : "Describe extra parameters.",
    "title" : "describeExtra(_:)"
  },
  {
    "headings" : [
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/modules()"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Return a flat array of all the Module in the instance (including self).  See Also namedModules() children() leafModules()",
    "summary" : "Return a flat array of all the Module in the instance (including self).",
    "title" : "modules()"
  },
  {
    "headings" : [
      "Discussion"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Linear\/init(_:_:bias:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies an affine transformation to the input.  number of input dimensions number of output dimensions if true this layer will apply a bias Discussion Please see discussion in Module.",
    "summary" : "Applies an affine transformation to the input.",
    "title" : "init(_:_:bias:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/rawValue"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from RawRepresentable.rawValue. ",
    "summary" : "Inherited from RawRepresentable.rawValue.",
    "title" : "rawValue"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Dropout\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/CELU\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "Return Value",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/tripletLoss(anchors:positives:negatives:axis:p:margin:eps:reduction:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Computes the triplet loss for a set of anchor, positive, and negative samples. Margin is represented with alpha in the math section.  anchor samples positive samples negative samples distribution axis norm dree for pairwise distance margin for the triplet loss small positive constant to prevent numerical instability reduction type Return Value Computed triplet loss. See Also Loss Functions",
    "summary" : "Computes the triplet loss for a set of anchor, positive, and negative samples. Margin is represented with alpha in the math section.",
    "title" : "tripletLoss(anchors:positives:negatives:axis:p:margin:eps:reduction:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/remove(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from OptionSet.remove(_:). ",
    "summary" : "Inherited from OptionSet.remove(_:).",
    "title" : "remove(_:)"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ModuleValue"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "A single value from Module.  Overview This is typically produced from items() or indirectly via filterMap(filter:map:isLeaf:). See Also items() filterMap(filter:map:isLeaf:) ModuleItems ModuleItem",
    "summary" : "A single value from Module.",
    "title" : "ModuleValue"
  },
  {
    "headings" : [
      "Discussion"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/QuantizedLinear\/init(_:_:bias:groupSize:bits:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies an affine transformation to the input using a quantized weight matrix.  number of input dimensions number of output dimensions if true this layer will apply a bias The group size to use for the quantized weight The bit width to use for the quantized weight Discussion This is the quantized version of Linear.  Typically this is used via quantize(model:groupSize:bits:predicate:).",
    "summary" : "Applies an affine transformation to the input using a quantized weight matrix.",
    "title" : "init(_:_:bias:groupSize:bits:)"
  },
  {
    "headings" : [
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ALiBi"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "  See Also Positional Encoding",
    "summary" : "",
    "title" : "ALiBi"
  },
  {
    "headings" : [
      "Return Value",
      "Discussion",
      "Other Arguments",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/valueAndGrad(model:_:)-12a2c"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Transform the passed function f(Model, MLXArray, MLXArray) to a function that computes the gradients of f with regard to the model’s trainable parameters and also its value.  model to apply parameters to function to compute the gradients for Return Value function that returns the value of f() and the gradient of the parameters of the model Discussion For example:  Other Arguments If other arguments to the loss function are needed there are two variants of valueAndGrad() that can be used to build these: valueAndGrad(model:_:) – passing only [MLXArray] and returning [MLXArray] valueAndGrad(model:_:)– passing any arguments and returning [MLXArray] Prefer the former as it can cache the value of the underlying valueAndGrad() call while the latter must rebuild it for each call. See Also Training a Model",
    "summary" : "Transform the passed function f(Model, MLXArray, MLXArray) to a function that computes the gradients of f with regard to the model’s trainable parameters and also its value.",
    "title" : "valueAndGrad(model:_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/parameters()"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Return a NestedDictionary<String, MLXArray> for all parameters in the model (all layers). ",
    "summary" : "Return a NestedDictionary<String, MLXArray> for all parameters in the model (all layers).",
    "title" : "parameters()"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/PReLU"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the element-wise parametric ReLU.  Overview This is:  See Also Activation Functions and Layers prelu(_:alpha:)",
    "summary" : "Applies the element-wise parametric ReLU.",
    "title" : "PReLU"
  },
  {
    "headings" : [
      "References",
      "See also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LayerNorm"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies layer normalization [1] on the inputs.  References https:\/\/arxiv.org\/abs\/1607.06450 See also Normalization",
    "summary" : "Applies layer normalization [1] on the inputs.",
    "title" : "LayerNorm"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/init()"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Initializes the module. ",
    "summary" : "Initializes the module.",
    "title" : "init()"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/PReLU\/init(count:value:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "init(count:value:)"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ModuleInfo"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "ModuleInfo can provde information about child modules and act as an update point for update(modules:verify:).  Overview The keys for modules and parameters are usually named after their instance variables, but feed_forward would not be a very Swifty variable name:  Instead we can use ModuleInfo to supply a replacement key that matches the python version:  All Linear modules should use a ModuleInfo so that quantize(model:groupSize:bits:predicate:) can replace them at runtime:  The ModuleInfo provides a hook for QuantizedLinear and update(modules:verify:) \/\/\/ to replace the contents of w1, etc. with a new compatible Model after it is created. See Also Creating Modules ParameterInfo",
    "summary" : "ModuleInfo can provde information about child modules and act as an update point for update(modules:verify:).",
    "title" : "ModuleInfo"
  },
  {
    "headings" : [
      "Neural Networks",
      "The Module Class",
      "Parameters",
      "Updating the Parameters",
      "Inspecting Modules",
      "ModuleInfo and ParameterInfo",
      "Converting From Python"
    ],
    "kind" : "article",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/custom-layers"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Creating custom modules using MLXNN. Neural Networks Writing arbitrarily complex neural networks in MLX can be done using only MLXArray and valueAndGrad().  However, this requires the user to write again and again the same simple neural network operations as well as handle all the parameter state and initialization manually and explicitly. The MLXNN package solves this problem by providing an intuitive way of composing neural network layers, initializing their parameters, freezing them for finetuning and more. The Module Class The workhorse of any neural network library is the Module class. In MLX the Module class is a container of MLXArray or Module instances. Its main function is to provide a way to recursively access and update its parameters and those of its submodules. Creating a new Module subclass from scratch looks like this:  This will declare a FeedForward layer similar to the layer in the Mistral Example. This layer can be used:  See the Converting From Python section about other considerations when converting code. Parameters A parameter of a module is any public member of type MLXArray (its name should not start with _). It can be arbitrarily nested in other Module instances or [MLXArray] and [String:MLXArray]. parameters() can be used to extract a NestedDictionary (ModuleParameters) with all the parameters of a module and its submodules. A Module can also keep track of “frozen” parameters. See the freeze(recursive:keys:strict:) method for more details. These parameters will not be considered when computing gradients and updating weights via valueAndGrad(model:_:). See the ModuleInfo and ParameterInfo section for more information about using these in swift. Updating the Parameters MLX modules allow accessing and updating individual parameters. However, most times we need to update large subsets of a module’s parameters. This action is performed by update(parameters:verify:). See also Training a Model. Inspecting Modules The simplest way to see the model architecture is to print it. Following along with the above example, you can print the FeedForward with:  This will display:  To get more detailed information on the arrays in a Module you can use mapParameters(map:isLeaf:).  For example to see the shapes of all the parameters from above:  resulting in:  ModuleInfo and ParameterInfo The ModuleInfo and ParameterInfo provide two important features for module instance variables: both property wrappers allow replacement keys to be specified the ModuleInfo allows update(modules:verify:) to replace the module Replacement keys are important because many times models and weights are defined in terms of their python implementation.  For example here is a definition of a module:  The keys for modules and parameters are usually named after their instance variables, but feed_forward would not be a very Swifty variable name.  Instead we can use ModuleInfo to supply a replacement key:  All Linear modules should use a ModuleInfo so that quantize(model:groupSize:bits:predicate:) can replace them at runtime:  The ModuleInfo provides a hook for QuantizedLinear and update(modules:verify:) to replace the contents of w1, etc. with a new compatible Model after it is created. Note that MLXArray is settable without any ParameterInfo – it has an update() method. Converting From Python Consider this example from a Llama model:  The straightforward conversion might look like this:  Here is another example that has parameters (MLXArray) from the mlx.nn package (both sans documentation):  and the swift conversion: ",
    "summary" : "Creating custom modules using MLXNN.",
    "title" : "Creating Modules"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/mapParameters(map:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Function that will turn a (MLXArray) -> Result? into a function suitable for use in filterMap(filter:map:isLeaf:).  Discussion  This is also trivially done with mapParameters(map:isLeaf:), which uses this function internally:  See Also Module Filter and Map Functions ModuleValue.parameters(_:) mapParameters(map:isLeaf:) filterMap(filter:map:isLeaf:)",
    "summary" : "Function that will turn a (MLXArray) -> Result? into a function suitable for use in filterMap(filter:map:isLeaf:).",
    "title" : "mapParameters(map:)"
  },
  {
    "headings" : [

    ],
    "kind" : "article",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/normalization"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Built-in normalization layers",
    "summary" : "Built-in normalization layers",
    "title" : "Normalization"
  },
  {
    "headings" : [
      "Discussion",
      "References"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/InstanceNorm\/init(dimensions:eps:affine:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies instance normalization [1] on the inputs.  number of features in the input value added to the denominator for numerical stability if true adds a trainable weight and bias Discussion See InstanceNorm python docs for more information. References https:\/\/arxiv.org\/abs\/1607.08022",
    "summary" : "Applies instance normalization [1] on the inputs.",
    "title" : "init(dimensions:eps:affine:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/noUnusedKeys"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Check that all keys are used.  This is useful to ensure that e.g. all loaded parameters are used – there are no names that don’t match. ",
    "summary" : "Check that all keys are used.  This is useful to ensure that e.g. all loaded parameters are used – there are no names that don’t match.",
    "title" : "noUnusedKeys"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SequentialBuilder\/buildOptional(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "buildOptional(_:)"
  },
  {
    "headings" : [
      "Discussion"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Conv1d\/init(inputChannels:outputChannels:kernelSize:stride:padding:bias:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies a 1-dimensional convolution over the multi-channel input sequence.  number of input channels (C from the discussion) number of output channels size of the convolution filters stride when applying the filter many positions to 0-pad the input with if true add a learnable bias to the output Discussion The channels are expected to be last i.e. the input shape should be NLC where: N is the batch dimension L is the sequence length C is the number of input channels",
    "summary" : "Applies a 1-dimensional convolution over the multi-channel input sequence.",
    "title" : "init(inputChannels:outputChannels:kernelSize:stride:padding:bias:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/isLeafDefault"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Default leaf filter – treat ModuleValue.parameters(_:) and ModuleValue.other(_:) as leaves.  Discussion This will allow recursion into .array, .dictionary and ModuleValue.module(_:). See Also Module Filter and Map Functions filterMap(filter:map:isLeaf:)",
    "summary" : "Default leaf filter – treat ModuleValue.parameters(_:) and ModuleValue.other(_:) as leaves.",
    "title" : "isLeafDefault"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/formUnion(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from OptionSet.formUnion(_:). ",
    "summary" : "Inherited from OptionSet.formUnion(_:).",
    "title" : "formUnion(_:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/softSign(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Softsign function.  Discussion This is:  See Also Activation Functions and Layers SoftSign",
    "summary" : "Applies the Softsign function.",
    "title" : "softSign(_:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/unfreeze(recursive:keys:strict:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Unfreeze the Module’s parameters or subset.  if true this will unfreeze the parameters of child Module recursively optional keys to unfreeze – if unspecified, will apply to all if true validate that the passed keys exist Discussion A frozen parameter does not compute gradients.  The function is idempotent – unfreezing a frozen model is a no-op. For instance to only train the biases of a Transformer one can do:  See Also freeze(recursive:keys:) unfreeze(recursive:keys:strict:)",
    "summary" : "Unfreeze the Module’s parameters or subset.",
    "title" : "unfreeze(recursive:keys:strict:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/CELU\/init(alpha:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "init(alpha:)"
  },
  {
    "headings" : [
      "Discussion"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Embedding\/init(embeddingCount:dimensions:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Implements a simple lookup table that maps each input integer to a high-dimensional vector.  How many possible discrete tokens can we embed.  Usually called the vocabulary size. dimensionality of the embeddings. Discussion Typically used to embed discrete tokens for processing by neural networks.",
    "summary" : "Implements a simple lookup table that maps each input integer to a high-dimensional vector.",
    "title" : "init(embeddingCount:dimensions:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/symmetricDifference(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from OptionSet.symmetricDifference(_:). ",
    "summary" : "Inherited from OptionSet.symmetricDifference(_:).",
    "title" : "symmetricDifference(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LayerNorm\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/geluFastApproximate(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "A fast approximation to Gaussian Error Linear Unit.  Discussion This is:  See Also Activation Functions and Layers GELU gelu(_:) geluApproximate(_:)",
    "summary" : "A fast approximation to Gaussian Error Linear Unit.",
    "title" : "geluFastApproximate(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/BatchNorm\/unfreeze(recursive:keys:strict:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from Module.unfreeze(recursive:keys:strict:). ",
    "summary" : "Inherited from Module.unfreeze(recursive:keys:strict:).",
    "title" : "unfreeze(recursive:keys:strict:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/children()"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Produces a NestedDictionary<String, Module> for all direct children of the module. ",
    "summary" : "Produces a NestedDictionary<String, Module> for all direct children of the module.",
    "title" : "children()"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ReLU\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/none"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "none"
  },
  {
    "headings" : [
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/filterTrainableParameters"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Filter that will accept all structure (.array and .dictionary) and ModuleValue.parameters(_:) or ModuleValue.module(_:) that are not in the noGrad set.  See Also Module Filter and Map Functions freeze(recursive:keys:strict:) filterValidParameters filterLocalParameters",
    "summary" : "Filter that will accept all structure (.array and .dictionary) and ModuleValue.parameters(_:) or ModuleValue.module(_:) that are not in the noGrad set.",
    "title" : "filterTrainableParameters"
  },
  {
    "headings" : [
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/filterValidChild"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Filter that will accept all structure (.array and .dictionary) and ModuleValue.module(_:).  See Also Module Filter and Map Functions",
    "summary" : "Filter that will accept all structure (.array and .dictionary) and ModuleValue.module(_:).",
    "title" : "filterValidChild"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SequentialBuilder\/buildArray(_:)-876fs"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "buildArray(_:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/freeze(recursive:keys:strict:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Freeze the Module’s parameters or subset.  if true this will freeze the parameters of child Module recursively optional keys tofreezeunfreeze – if unspecified, will apply to all if true validate that the passed keys exist Discussion A frozen parameter does not compute gradients.  The function is idempotent – freezing a frozen model is a no-op. For example to only train the attention parameters from a Transformer:  See Also freeze(recursive:keys:) unfreeze(recursive:keys:strict:)",
    "summary" : "Freeze the Module’s parameters or subset.",
    "title" : "freeze(recursive:keys:strict:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/isDisjoint(with:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from SetAlgebra.isDisjoint(with:). ",
    "summary" : "Inherited from SetAlgebra.isDisjoint(with:).",
    "title" : "isDisjoint(with:)"
  },
  {
    "headings" : [
      "Overview",
      "References",
      "See also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/RMSNorm"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies Root Mean Square normalization [1] to the inputs.  Overview Concretely:  where weight is initialized with ones and eps is a small float to ensure the numerical stability of inverse square root. References https:\/\/arxiv.org\/abs\/1910.07467 See also Normalization",
    "summary" : "Applies Root Mean Square normalization [1] to the inputs.",
    "title" : "RMSNorm"
  },
  {
    "headings" : [

    ],
    "kind" : "article",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/transformers"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Built-in transformer layers",
    "summary" : "Built-in transformer layers",
    "title" : "Transformer"
  },
  {
    "headings" : [
      "Return Value",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/crossEntropy(logits:targets:weights:axis:labelSmoothing:reduction:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Computes the cross entropy loss.  unnormalized predicted logits target values, as class indices weights for each target axis over which to compute softmax label smoothing factor, range [0, 1) reduction type Return Value computed cross entropy loss See Also Loss Functions",
    "summary" : "Computes the cross entropy loss.",
    "title" : "crossEntropy(logits:targets:weights:axis:labelSmoothing:reduction:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/RoPE\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Evaluate with offset of 0. ",
    "summary" : "Evaluate with offset of 0.",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/logSigmoid(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Log Sigmoid function.  Discussion This is:  See Also Activation Functions and Layers LogSigmoid",
    "summary" : "Applies the Log Sigmoid function.",
    "title" : "logSigmoid(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Tanh\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Step\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "Return Value",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/l1Loss(predictions:targets:reduction:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Computes the L1 loss  the predicted values the target values reduction type Return Value computed L1 loss See Also Loss Functions",
    "summary" : "Computes the L1 loss",
    "title" : "l1Loss(predictions:targets:reduction:)"
  },
  {
    "headings" : [
      "Return Value",
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/logCoshLoss(inputs:targets:reduction:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Computes the log cosh loss between inputs and targets.  predicted values target values reduction type Return Value computed log cosh loss Discussion Logcosh acts like L2 loss for small errors, ensuring stable gradients, and like the L1 loss for large errors, reducing sensitivity to outliers. This dual behavior offers a balanced, robust approach for regression tasks. See Also Loss Functions",
    "summary" : "Computes the log cosh loss between inputs and targets.",
    "title" : "logCoshLoss(inputs:targets:reduction:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ParameterInfo\/init(key:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "init(key:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Dropout\/init(p:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "init(p:)"
  },
  {
    "headings" : [
      "Discussion"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ModuleValue\/other(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "A non-MLXArray and non-Module value.  Discussion From code: ",
    "summary" : "A non-MLXArray and non-Module value.",
    "title" : "ModuleValue.other(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LeakyReLU\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/update(with:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from OptionSet.update(with:). ",
    "summary" : "Inherited from OptionSet.update(with:).",
    "title" : "update(with:)"
  },
  {
    "headings" : [
      "Return Value"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/QuantizedLinear\/from(linear:groupSize:bits:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Returns a QuantizedLinear layer that applies the same linear transformation up to the quantization error.  a Linear layer The group size to use for the quantized weight The bit width to use for the quantized weight Return Value a new QuantizedLayer",
    "summary" : "Returns a QuantizedLinear layer that applies the same linear transformation up to the quantization error.",
    "title" : "from(linear:groupSize:bits:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Bilinear\/init(_:_:_:bias:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "init(_:_:_:bias:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ModuleChilren"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "NestedDictionary structure of Module ",
    "summary" : "NestedDictionary structure of Module",
    "title" : "ModuleChilren"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/MultiHeadAttention\/callAsFunction(_:keys:values:mask:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "callAsFunction(_:keys:values:mask:)"
  },
  {
    "headings" : [

    ],
    "kind" : "article",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/IndentedDescription-Implementations"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "",
    "summary" : "",
    "title" : "IndentedDescription Implementations"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SequentialBuilder\/buildEither(second:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "buildEither(second:)"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Step"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Step Activation Function.  Overview This function implements a binary step activation, where the output is set to 1 if the input is greater than a specified threshold, and 0 otherwise. This is:  See Also Activation Functions and Layers step(_:threshold:)",
    "summary" : "Applies the Step Activation Function.",
    "title" : "Step"
  },
  {
    "headings" : [
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/isLeafModule"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Leaf filter that will stop at ModuleValue.module(_:) and recurse into all other structure.  See Also Module Filter and Map Functions filterMap(filter:map:isLeaf:)",
    "summary" : "Leaf filter that will stop at ModuleValue.module(_:) and recurse into all other structure.",
    "title" : "isLeafModule"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/CELU"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Continuously Differentiable Exponential Linear Unit.  Overview This is:  See Also Activation Functions and Layers celu(_:alpha:)",
    "summary" : "Applies the Continuously Differentiable Exponential Linear Unit.",
    "title" : "CELU"
  },
  {
    "headings" : [

    ],
    "kind" : "article",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/positional-encoding"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Built-in layers for positional encoding.",
    "summary" : "Built-in layers for positional encoding.",
    "title" : "Positional Encoding"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/GLU\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/geluApproximate(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "An approximation to Gaussian Error Linear Unit.  Discussion This is:  See Also Activation Functions and Layers GELU gelu(_:) geluFastApproximate(_:)",
    "summary" : "An approximation to Gaussian Error Linear Unit.",
    "title" : "geluApproximate(_:)"
  },
  {
    "headings" : [
      "References",
      "See also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/BatchNorm"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies batch normalization [1] on the inputs.  References https:\/\/arxiv.org\/abs\/1502.03167 See also Normalization",
    "summary" : "Applies batch normalization [1] on the inputs.",
    "title" : "BatchNorm"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/relu(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Rectified Linear Unit.  Discussion This is:  See Also Activation Functions and Layers ReLU",
    "summary" : "Applies the Rectified Linear Unit.",
    "title" : "relu(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SequentialBuilder\/buildEither(first:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "buildEither(first:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/union(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from OptionSet.union(_:). ",
    "summary" : "Inherited from OptionSet.union(_:).",
    "title" : "union(_:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/update(parameters:verify:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Replace the parameters of this Module with the provided parameters.  replacement parameters in the same format that parameters() or mapParameters(map:isLeaf:) provides options for verifying parameters Discussion This will replace the parameters in the Module recursively with the given ModuleParameters structure.  For example:  The parameters need not provide all values in the model – any omitted values will be unchanged. The apply(filter:map:) can be used for similar purposes to apply changes in-place. See Also Creating Modules update(parameters:) apply(filter:map:) parameters() mapParameters(map:isLeaf:) update(modules:verify:)",
    "summary" : "Replace the parameters of this Module with the provided parameters.",
    "title" : "update(parameters:verify:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/GELU\/Approximation\/precise"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "See geluApproximate(_:) ",
    "summary" : "See geluApproximate(_:)",
    "title" : "GELU.Approximation.precise"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/contains(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from OptionSet.contains(_:). ",
    "summary" : "Inherited from OptionSet.contains(_:).",
    "title" : "contains(_:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/logSoftMax(_:axis:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Log Softmax function.  Discussion This is:  See Also Activation Functions and Layers LogSoftMax",
    "summary" : "Applies the Log Softmax function.",
    "title" : "logSoftMax(_:axis:)"
  },
  {
    "headings" : [
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/isLeafModuleNoChildren"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Leaf filter that will stop at ModuleValue.module(_:) if they have no child modules and recurse into all other structure.  See Also Module Filter and Map Functions filterMap(filter:map:isLeaf:)",
    "summary" : "Leaf filter that will stop at ModuleValue.module(_:) if they have no child modules and recurse into all other structure.",
    "title" : "isLeafModuleNoChildren"
  },
  {
    "headings" : [
      "Discussion"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ModuleValue\/fromMirror(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Return (String, ModuleItem) tuple or nil if the label cannot be determined.  Discussion Called from items()",
    "summary" : "Return (String, ModuleItem) tuple or nil if the label cannot be determined.",
    "title" : "fromMirror(_:)"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SoftMax"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Softmax function.  Overview This is:  See Also Activation Functions and Layers",
    "summary" : "Applies the Softmax function.",
    "title" : "SoftMax"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/sigmoid(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the element-wise logistic sigmoid.  Discussion For details, please see this documentation This is:  See Also Activation Functions and Layers Sigmoid",
    "summary" : "Applies the element-wise logistic sigmoid.",
    "title" : "sigmoid(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/formSymmetricDifference(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from OptionSet.formSymmetricDifference(_:). ",
    "summary" : "Inherited from OptionSet.formSymmetricDifference(_:).",
    "title" : "formSymmetricDifference(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/GELU\/Approximation\/fast"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "See geluFastApproximate(_:) ",
    "summary" : "See geluFastApproximate(_:)",
    "title" : "GELU.Approximation.fast"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SoftSign"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Softsign function.  Overview This is:  See Also Activation Functions and Layers softSign(_:)",
    "summary" : "Applies the Softsign function.",
    "title" : "SoftSign"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/mapOther(map:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Function that will turn a (Any) -> Result? into a function suitable for use in filterMap(filter:map:isLeaf:).  Discussion For example:  See Also Module Filter and Map Functions ModuleValue.other(_:) filterMap(filter:map:isLeaf:)",
    "summary" : "Function that will turn a (Any) -> Result? into a function suitable for use in filterMap(filter:map:isLeaf:).",
    "title" : "mapOther(map:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/subtracting(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from SetAlgebra.subtracting(_:). ",
    "summary" : "Inherited from SetAlgebra.subtracting(_:).",
    "title" : "subtracting(_:)"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Bilinear"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies a bilinear transformation to the inputs.  Overview Concretely:  where w has shape [outputDimensions, inputDimensions2, inputDimensions1] and b has shape [outputDimensions]. The values are initialized from the uniform distribution:  See Also Creating Modules Linear",
    "summary" : "Applies a bilinear transformation to the inputs.",
    "title" : "Bilinear"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/QuantizedLinear\/init(weight:bias:groupSize:bits:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "init(weight:bias:groupSize:bits:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LossReduction\/hash(into:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from RawRepresentable.hash(into:). ",
    "summary" : "Inherited from RawRepresentable.hash(into:).",
    "title" : "hash(into:)"
  },
  {
    "headings" : [
      "Return Value",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/hingeLoss(inputs:targets:reduction:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Computes the hinge loss between inputs and targets.  predicted values target values: -1 or 1 reduction type Return Value computed hinge loss See Also Loss Functions",
    "summary" : "Computes the hinge loss between inputs and targets.",
    "title" : "hingeLoss(inputs:targets:reduction:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Bilinear\/describeExtra(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Describe the inputDimensions and outputDimensions. ",
    "summary" : "Describe the inputDimensions and outputDimensions.",
    "title" : "describeExtra(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/training"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Flag to indicate whether the module is being trained.  Manipulated via train(_:). ",
    "summary" : "Flag to indicate whether the module is being trained.  Manipulated via train(_:).",
    "title" : "training"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Tanh"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the hyperbolic tangent function ",
    "summary" : "Applies the hyperbolic tangent function",
    "title" : "Tanh"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/BatchNorm\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/filterValidParameters"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Filter that will accept all structure (.array and .dictionary) and ModuleValue.parameters(_:) or ModuleValue.module(_:) allowing recursion into sub-Modules (layers).  See Also Module Filter and Map Functions filterLocalParameters filterTrainableParameters",
    "summary" : "Filter that will accept all structure (.array and .dictionary) and ModuleValue.parameters(_:) or ModuleValue.module(_:) allowing recursion into sub-Modules (layers).",
    "title" : "filterValidParameters"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/PReLU\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "Discussion"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/apply(filter:map:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Apply a closure to the parameters in a Module recursively.  filter for parameters to apply to function to apply to the matched parameters Discussion For example to change all floating point parameters to DType.float16: ",
    "summary" : "Apply a closure to the parameters in a Module recursively.",
    "title" : "apply(filter:map:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/isSuperset(of:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from SetAlgebra.isSuperset(of:). ",
    "summary" : "Inherited from SetAlgebra.isSuperset(of:).",
    "title" : "isSuperset(of:)"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Dropout3d"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Apply 3D channel-wise dropout during training.  Overview Randomly zero out entire channels independently with probability p. This layer expects the channels to be last, i.e., the input shape should be NDHWC or DHWC where: N is the batch dimension, D is the depth, H is the input image height, W is the input image width, and C is the number of input channels. The remaining channels are scaled by 1 \/ (1-p) to maintain the expected value of each element. Unlike traditional dropout, which zeros individual entries, this layer zeros entire channels. This is often beneficial for convolutional layers processing 3D data, like in medical imaging or video processing. See Also Dropout Dropout2d",
    "summary" : "Apply 3D channel-wise dropout during training.",
    "title" : "Dropout3d"
  },
  {
    "headings" : [

    ],
    "kind" : "article",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/OptionSet-Implementations"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "",
    "summary" : "",
    "title" : "OptionSet Implementations"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Step\/init(threshold:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "init(threshold:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/noGrad"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Set of property names that are frozen.  Maniupulated via freeze(recursive:keys:strict:) and unfreeze(recursive:keys:strict:). ",
    "summary" : "Set of property names that are frozen.  Maniupulated via freeze(recursive:keys:strict:) and unfreeze(recursive:keys:strict:).",
    "title" : "noGrad"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Conv2d\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "Return Value",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/huberLoss(inputs:targets:delta:reduction:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Computes the Huber loss between inputs and targets.  predicted values target values threshold at which to change between L1 and L2 loss reduction type Return Value computed Huber loss See Also Loss Functions",
    "summary" : "Computes the Huber loss between inputs and targets.",
    "title" : "huberLoss(inputs:targets:delta:reduction:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ModuleItem"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Single item from items() ",
    "summary" : "Single item from items()",
    "title" : "ModuleItem"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ReLU6"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Rectified Linear Unit 6.  Overview This is:  See Also Activation Functions and Layers relu6(_:)",
    "summary" : "Applies the Rectified Linear Unit 6.",
    "title" : "ReLU6"
  },
  {
    "headings" : [
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/filterAll"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Filter that will accept all values.  See Also Module Filter and Map Functions",
    "summary" : "Filter that will accept all values.",
    "title" : "filterAll"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/!=(_:_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from Equatable.!=(_:_:). ",
    "summary" : "Inherited from Equatable.!=(_:_:).",
    "title" : "!=(_:_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SinusoidalPositionalEncoding\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SequentialBuilder\/buildExpression(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "buildExpression(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SequentialBuilder\/buildPartialBlock(first:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "buildPartialBlock(first:)"
  },
  {
    "headings" : [
      "Return Value",
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/filterMap(filter:map:isLeaf:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Recursively filter and map the contents of the module and its children and produce a NestedDictionary with the results.  filter function that determines if the (Module, Key, Item) tuple should be examined Transformation of the values.  By default this is identity but the caller can transform to other types, etc.  See mapParameters(map:) and others for helper functions that can assist with common types. closure that determines if a value is a leaf or not. Return Value NestedDictionary matching the structure with mapped values. Discussion Traverses the ModuleItems produced by items() and filters and maps their contents.  For each item in the ModuleItems this will call the filter to determine if it should be included.  There are a number of predefined filters available, see Module Filter and Map Functions.  filterAll will accept all values while filterValidParameters will only examine structure and parameters. The map function transforms the visited values.  By default it is identity and will just return the ModuleItem directly.  There are a number of helper functions like mapParameters(map:) and mapModule(map:) that can help deal with types like MLXArray or Module.  For example:  The isLeaf function is called to determine if the value should be transformed via the map or if the structure should be traversed.  For example this will collect the leaf modules:  See Also Module Filter and Map Functions parameters() mapParameters(map:isLeaf:) modules() items()",
    "summary" : "Recursively filter and map the contents of the module and its children and produce a NestedDictionary with the results.",
    "title" : "filterMap(filter:map:isLeaf:)"
  },
  {
    "headings" : [
      "Return Value",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/mseLoss(predictions:targets:reduction:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Computes the mean squared error loss.  the predicted values the target values reduction type Return Value computed mean squared error loss See Also Loss Functions",
    "summary" : "Computes the mean squared error loss.",
    "title" : "mseLoss(predictions:targets:reduction:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Linear\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/RoPE\/callAsFunction(_:offset:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "callAsFunction(_:offset:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Linear\/describeExtra(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Describe the inputDimensions and outputDimensions. ",
    "summary" : "Describe the inputDimensions and outputDimensions.",
    "title" : "describeExtra(_:)"
  },
  {
    "headings" : [
      "Discussion"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/update(modules:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "A non-throwing version of update(modules:verify:).  Discussion This passes verify: .none.  Note that there may still be fatalErrors() if for example an Module is set on a MLXArray.",
    "summary" : "A non-throwing version of update(modules:verify:).",
    "title" : "update(modules:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Dropout2d\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LossReduction"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Different types of loss reductions ",
    "summary" : "Different types of loss reductions",
    "title" : "LossReduction"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/update(modules:verify:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Replace the child modules of this Module with the provided replacements.  replacement modules in the same format as children() or leafModules() options for verifying parameters Discussion This will replace the parameters in the Module recursively with the given ModuleChilren structure.  For example this is typically called via a helper function:  Note that the modules being replace must use a ModuleInfo property wrapper – this provides the mechanism to update the values.  Also note that the replacement models must be assignable to the ivar’s type. For example:  Would be able to be replaced with quantize(model:groupSize:bits:predicate:). The modules need not provide all values in the model – any omitted values will be unchanged. See Also Creating Modules update(modules:) update(parameters:verify:) children() leafModules() quantize(model:groupSize:bits:predicate:)",
    "summary" : "Replace the child modules of this Module with the provided replacements.",
    "title" : "update(modules:verify:)"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LeakyReLU"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Leaky Rectified Linear Unit.  Overview This is:  See Also Activation Functions and Layers leakyRelu(_:negativeSlope:)",
    "summary" : "Applies the Leaky Rectified Linear Unit.",
    "title" : "LeakyReLU"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Bilinear\/callAsFunction(_:_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "callAsFunction(_:_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LossReduction\/mean"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "take the mean of the loss. This produces a a scalar array ",
    "summary" : "take the mean of the loss. This produces a a scalar array",
    "title" : "LossReduction.mean"
  },
  {
    "headings" : [

    ],
    "kind" : "article",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/Updatable-Implementations"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "",
    "summary" : "",
    "title" : "Updatable Implementations"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/silu(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Sigmoid Linear Unit. Also known as Swish.  Discussion This is:  See Also Activation Functions and Layers SiLU",
    "summary" : "Applies the Sigmoid Linear Unit. Also known as Swish.",
    "title" : "silu(_:)"
  },
  {
    "headings" : [
      "Overview"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Embedding"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Implements a simple lookup table that maps each input integer to a high-dimensional vector.  Overview Typically used to embed discrete tokens for processing by neural networks.",
    "summary" : "Implements a simple lookup table that maps each input integer to a high-dimensional vector.",
    "title" : "Embedding"
  },
  {
    "headings" : [
      "Overview"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SequentialBuilder"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "A way to build Sequential.  Overview See init(layers:)",
    "summary" : "A way to build Sequential.",
    "title" : "SequentialBuilder"
  },
  {
    "headings" : [
      "Overview",
      "Using In A Module",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Linear"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies an affine transformation to the input.  Overview Concretely:  where W has shape [inputDimensions, outputDimensions] and b has shape [outputDimensions]. The values are initialized from the uniform distribution:  Using In A Module Use @ModuleInfo with all your Linear module uses so that update(modules:verify:) can replace the modules, e.g. via quantize(model:groupSize:bits:predicate:). For example:  If a key is needed (to change the parameters key for parameters()) here is the way to initialize the ivar:  See Also Creating Modules QuantizedLinear Bilinear",
    "summary" : "Applies an affine transformation to the input.",
    "title" : "Linear"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/unfreeze(recursive:keys:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Non-throwing variant of unfreeze(recursive:keys:strict:) (strict: false). ",
    "summary" : "Non-throwing variant of unfreeze(recursive:keys:strict:) (strict: false).",
    "title" : "unfreeze(recursive:keys:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Dropout2d\/init(p:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "init(p:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/MultiHeadAttention\/init(dimensions:numHeads:queryInputDimensions:keyInputDimensions:valueInputDimensions:valueDimensions:valueOutputDimensions:bias:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Implements the scaled dot product attention with multiple heads.  model dimensions and default for the other dimensions if they are not supplied number of attention heads input dimensions of queries input dimensions of keys input dimensions of values dimensions of values after the projection dimensions new values will be projected to if true uses a bias in the Linear layers Discussion Given inputs for queries, keys and values the MultiHeadAttention produces new values by aggregating information from the input values according to the similarities of the input queries and keys. All inputs as well as the output are linearly projected without biases by default. MultiHeadAttention also takes an optional additive attention mask that should be broadcastable with (batch, numHeads, # queries, # keys). The mask should have -inf or very large negative numbers at the positions that should not be attended to. See Also ``createAdditiveCausalMask(_:dtype:)",
    "summary" : "Implements the scaled dot product attention with multiple heads.",
    "title" : "init(dimensions:numHeads:queryInputDimensions:keyInputDimensions:valueInputDimensions:valueDimensions:valueOutputDimensions:bias:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LogSoftMax\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/GELU\/Approximation"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "GELU.Approximation"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/subtract(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from SetAlgebra.subtract(_:). ",
    "summary" : "Inherited from SetAlgebra.subtract(_:).",
    "title" : "subtract(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/init(arrayLiteral:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from SetAlgebra.init(arrayLiteral:). ",
    "summary" : "Inherited from SetAlgebra.init(arrayLiteral:).",
    "title" : "init(arrayLiteral:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ALiBi\/callAsFunction(attentionScores:offset:mask:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "callAsFunction(attentionScores:offset:mask:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/QuantizedLinear\/unfreeze(recursive:keys:strict:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from Module.unfreeze(recursive:keys:strict:). ",
    "summary" : "Inherited from Module.unfreeze(recursive:keys:strict:).",
    "title" : "unfreeze(recursive:keys:strict:)"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Mish"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Mish function, element-wise.  Overview Mish: A Self Regularized Non-Monotonic Neural Activation Function. Reference: https:\/\/arxiv.org\/abs\/1908.08681 This is:  See Also Activation Functions and Layers mish(_:)",
    "summary" : "Applies the Mish function, element-wise.",
    "title" : "Mish"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/glu(_:axis:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the gated linear unit function.  Discussion This function splits the axis dimension of the input into two halves (a and b) and applies a * sigmoid(b). See Also Activation Functions and Layers GLU",
    "summary" : "Applies the gated linear unit function.",
    "title" : "glu(_:axis:)"
  },
  {
    "headings" : [

    ],
    "kind" : "article",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LossReduction\/RawRepresentable-Implementations"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "",
    "summary" : "",
    "title" : "RawRepresentable Implementations"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/ModuleInfo\/wrappedValue"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "wrappedValue"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/RMSNorm\/init(dimensions:eps:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "init(dimensions:eps:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SiLU\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/GELU\/Approximation\/none"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "See gelu(_:) ",
    "summary" : "See gelu(_:)",
    "title" : "GELU.Approximation.none"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Transformer\/callAsFunction(source:target:sourceMask:targetMask:memoryMask:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "callAsFunction(source:target:sourceMask:targetMask:memoryMask:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Dropout3d\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/elu(_:alpha:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Exponential Linear Unit.  Discussion This is:  See Also Activation Functions and Layers",
    "summary" : "Applies the Exponential Linear Unit.",
    "title" : "elu(_:alpha:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/isStrictSuperset(of:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from SetAlgebra.isStrictSuperset(of:). ",
    "summary" : "Inherited from SetAlgebra.isStrictSuperset(of:).",
    "title" : "isStrictSuperset(of:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/celu(_:alpha:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Continuously Differentiable Exponential Linear Unit.  Discussion This is:  See Also Activation Functions and Layers CELU",
    "summary" : "Applies the Continuously Differentiable Exponential Linear Unit.",
    "title" : "celu(_:alpha:)"
  },
  {
    "headings" : [
      "Return Value",
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/smoothL1Loss(predictions:targets:beta:reduction:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Computes the smooth L1 loss.  predicted values ground truth values threshold after which the loss changes from the squared to the absolute difference reduction type Return Value computed smooth L1 loss Discussion The smooth L1 loss is a variant of the L1 loss which replaces the absolute difference with a squared difference when the absolute difference is less than beta. See Also Loss Functions",
    "summary" : "Computes the smooth L1 loss.",
    "title" : "smoothL1Loss(predictions:targets:beta:reduction:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Linear\/shape"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "shape"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Sequential\/init(layers:)-2rde0"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "init(layers:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/freeze(recursive:keys:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Non-throwing variant of freeze(recursive:keys:strict:) (strict: false). ",
    "summary" : "Non-throwing variant of freeze(recursive:keys:strict:) (strict: false).",
    "title" : "freeze(recursive:keys:)"
  },
  {
    "headings" : [
      "Discussion",
      "References"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/BatchNorm\/init(featureCount:eps:momentum:affine:trackRunningStats:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies batch normalization [1] on the inputs.  number of features in the input value added to the denominator for numerical stability momentum for updating the running mean and variance if true adds a trainable weight and bias if true track the running mean and variance Discussion See BatchNorm python docs for more information. The input shape is specified as NC or NLC, where N is the batch, C is the number of features or channels, and L is the sequence length. The output has the same shape as the input. For four-dimensional arrays, the shape is NHWC, where H and W are the height and width respectively. For more information on Batch Normalization, see the original paper “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift” https:\/\/arxiv.org\/abs\/1502.03167. References https:\/\/arxiv.org\/abs\/1502.03167",
    "summary" : "Applies batch normalization [1] on the inputs.",
    "title" : "init(featureCount:eps:momentum:affine:trackRunningStats:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LossReduction\/none"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "take the loss as-is.  This produces an array the same shape as the input. ",
    "summary" : "take the loss as-is.  This produces an array the same shape as the input.",
    "title" : "LossReduction.none"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/step(_:threshold:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Step Activation Function.  Discussion This function implements a binary step activation, where the output is set to 1 if the input is greater than a specified threshold, and 0 otherwise. This is:  See Also Activation Functions and Layers Step",
    "summary" : "Applies the Step Activation Function.",
    "title" : "step(_:threshold:)"
  },
  {
    "headings" : [
      "References",
      "See also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/GroupNorm"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies Group Normalization [1] on the inputs.  References https:\/\/arxiv.org\/abs\/1803.08494 See also Normalization",
    "summary" : "Applies Group Normalization [1] on the inputs.",
    "title" : "GroupNorm"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/softPlus(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Softplus function.  Discussion This is:  See Also Activation Functions and Layers SoftPlus",
    "summary" : "Applies the Softplus function.",
    "title" : "softPlus(_:)"
  },
  {
    "headings" : [
      "Overview"
    ],
    "kind" : "article",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/activations"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Built-in activation functions and layers. Overview MLXNN provides a number of activation functions and modules.  The modules simply wrap the functions, though some like GELU provide some settings that select between different functions.  Others, like CELU encapsulate parameters such as alpha.",
    "summary" : "Built-in activation functions and layers.",
    "title" : "Activation Functions and Layers"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/UnaryLayer"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "A Layer (Module subclass) that can be evaluated as a unary function.  Overview This provides callAsFunction(_:) with a single MLXArray input and a single MLXArray output. See Also Layers Sequential",
    "summary" : "A Layer (Module subclass) that can be evaluated as a unary function.",
    "title" : "UnaryLayer"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/SELU"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the Scaled Exponential Linear Unit.  Overview This is:  See Also Activation Functions and Layers selu(_:)",
    "summary" : "Applies the Scaled Exponential Linear Unit.",
    "title" : "SELU"
  },
  {
    "headings" : [
      "Overview",
      "Modules",
      "Parameters",
      "Training",
      "Other MLX Packages"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Neural Networks support for MLX Overview Writing arbitrarily complex neural networks in MLX can be done using only MLXArray and valueAndGrad(). However, this requires the user to write again and again the same simple neural network operations as well as handle all the parameter state and initialization manually and explicitly. The MLXNN package solves this problem by providing an intuitive way of composing neural network layers, initializing their parameters, freezing them for finetuning and more. Modules The workhorse of any neural network library is the Module class. In MLX the Module class is a container of MLXArray or Module instances. Its main function is to provide a way to recursively access and update its parameters and those of its submodules. Module Creating Modules Parameters A parameter of a module is any member of type MLXArray (its name should not start with _). It can be nested in other Module instances or Array and Dictionary. parameters() can be used to extract a NestedDictionary (ModuleParameters) with all the parameters of a module and its submodules. A Module can also keep track of “frozen” parameters. See the freeze(recursive:keys:strict:) method for more details. valueAndGrad(model:_:) the gradients returned will be with respect to these trainable parameters. Training See Training a Model Other MLX Packages MLX MLXRandom MLXNN MLXOptimizers MLXFFT MLXLinalg Python mlx",
    "summary" : "Neural Networks support for MLX",
    "title" : "MLXNN"
  },
  {
    "headings" : [
      "Discussion"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/trainableParameters()"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Return a NestedDictionary<String, MLXArray> for all trainable parameters in the model (all layers).  Discussion This omits freeze(recursive:keys:strict:) (frozen) parameters.",
    "summary" : "Return a NestedDictionary<String, MLXArray> for all trainable parameters in the model (all layers).",
    "title" : "trainableParameters()"
  },
  {
    "headings" : [
      "Overview",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Dropout2d"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Apply 2D channel-wise dropout during training.  Overview Randomly zero out entire channels independently with probability p. This layer expects the channels to be last, i.e. the input shape should be NWHC or WHC where:N is the batch dimension,H is the input image height,W is the input image width, andC is the number of input channels The remaining channels are scaled by 1 \/ (1-p) to maintain the expected value of each element. Unlike traditional dropout, which zeros individual entries, this layer zeros entire channels. This is beneficial for early convolution layers where adjacent pixels are correlated. In such case, traditional dropout may not effectively regularize activations. For more details, see [1]. [1]: Thompson, J., Goroshin, R., Jain, A., LeCun, Y. and Bregler C., 2015. Efficient Object Localization Using Convolutional Networks. CVPR 2015. See Also Dropout Dropout3d",
    "summary" : "Apply 2D channel-wise dropout during training.",
    "title" : "Dropout2d"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/init(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from SetAlgebra.init(_:). ",
    "summary" : "Inherited from SetAlgebra.init(_:).",
    "title" : "init(_:)"
  },
  {
    "headings" : [
      "Discussion",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/prelu(_:alpha:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Applies the element-wise parametric ReLU.  Discussion This is:  See Also Activation Functions and Layers PReLU",
    "summary" : "Applies the element-wise parametric ReLU.",
    "title" : "prelu(_:alpha:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/LossReduction\/!=(_:_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from Equatable.!=(_:_:). ",
    "summary" : "Inherited from Equatable.!=(_:_:).",
    "title" : "!=(_:_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Identity\/callAsFunction(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from UnaryLayer.callAsFunction(_:). ",
    "summary" : "Inherited from UnaryLayer.callAsFunction(_:).",
    "title" : "callAsFunction(_:)"
  },
  {
    "headings" : [
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/leafModules()"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Produces a NestedDictionary<String, Module> for all leaf modules module.  See Also isLeafModuleNoChildren",
    "summary" : "Produces a NestedDictionary<String, Module> for all leaf modules module.",
    "title" : "leafModules()"
  },
  {
    "headings" : [
      "Discussion"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/items()"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Return a NestedDictionary structure of ModuleItem representing the ivars of the Module instance.  Discussion This is typically not used directly – it is part of the implementation of filterMap(filter:map:isLeaf:) and update(parameters:) for example. Subclasses could potentially override this to provide custom introspection.",
    "summary" : "Return a NestedDictionary structure of ModuleItem representing the ivars of the Module instance.",
    "title" : "items()"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module\/VerifyUpdate\/formIntersection(_:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Inherited from OptionSet.formIntersection(_:). ",
    "summary" : "Inherited from OptionSet.formIntersection(_:).",
    "title" : "formIntersection(_:)"
  },
  {
    "headings" : [

    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/GELU\/init(approximation:)"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : " ",
    "summary" : "",
    "title" : "init(approximation:)"
  },
  {
    "headings" : [

    ],
    "kind" : "article",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/losses"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Built-in loss functions",
    "summary" : "Built-in loss functions",
    "title" : "Loss Functions"
  },
  {
    "headings" : [
      "Overview",
      "Parameters",
      "Example",
      "Training",
      "Mutation",
      "See Also"
    ],
    "kind" : "symbol",
    "location" : {
      "reference" : {
        "interfaceLanguage" : "swift",
        "url" : "doc:\/\/mlx.swift.mlxnn\/documentation\/MLXNN\/Module"
      },
      "type" : "topLevelPage"
    },
    "rawIndexableTextContent" : "Base class for building neural networks with MLX.  Overview The workhorse of any neural network library is the Module class. In MLX the Module class is a container of MLXArray or Module instances. Its main function is to provide a way to recursively access and update its parameters and those of its submodules. All the layers provided in Layers subclass this class and your models should do the same.  See Creating Modules. Parameters A Module can contain other Module instances or MLXArray instances in structures of Array and Dictionary. The Module then allows recursively extracting all the MLXArray instances using parameters() In addition, the Module has the concept of trainable and non trainable parameters (called “frozen”). When using valueAndGrad() or grad() the gradients are returned only with respect to the trainable parameters. All arrays in a module are trainable unless they are added in the “frozen” set by calling freeze(recursive:keys:strict:). valueAndGrad(model:_:) the gradients returned will be with respect to these trainable parameters. Example Here is an example multi-layer perception (MLP):  Please read Creating Modules for more information about implementing custom layers including how to override the module and parameter keys and allowing dynamic updates of the module structure to occur via update(modules:verify:). Training See Training a Model Mutation All mutation of parameters and modules must go through update(parameters:) and update(modules:).  This is important because Module uses reflection (Mirror) to find paramters and modules but caches the values.  These two methods make sure the cache is kept up-to-date. See Also Creating Modules",
    "summary" : "Base class for building neural networks with MLX.",
    "title" : "Module"
  }
]